{"cells":[{"cell_type":"markdown","metadata":{"id":"_yz-QM1sLt0C"},"source":["# Rock Paper Scissors (using Convolutional Neural Network)"]},{"cell_type":"markdown","metadata":{"id":"N_Z7Srd2miBD"},"source":["> - ü§ñ See [full list of Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) on **GitHub**<br/><br/>\n","> - ‚ñ∂Ô∏è **Interactive Demo**: [try this model and other machine learning experiments in action](https://trekhleb.github.io/machine-learning-experiments/)"]},{"cell_type":"markdown","metadata":{"id":"Q8lENOmkmiBF"},"source":["## Experiment overview"]},{"cell_type":"markdown","metadata":{"id":"L-ACu_RnmiBG"},"source":["In this experiment we will build a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) model using [Tensorflow](https://www.tensorflow.org/) to recognize Rock-Paper-Scissors signs (gestures) on the photo.\n","\n","A **convolutional neural network** (CNN, or ConvNet) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n","\n","![rock_paper_scissors_cnn.jpg](https://github.com/trekhleb/machine-learning-experiments/blob/master/demos/src/images/rock_paper_scissors_cnn.jpg?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"-eWIET19nffP"},"source":["_Inspired by [Getting started with TensorFlow 2.0](https://medium.com/@himanshurawlani/getting-started-with-tensorflow-2-0-faf5428febae) article._"]},{"cell_type":"markdown","metadata":{"id":"LoT5RmjYkt4J"},"source":["## Importing dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhMhFCmtNCpW"},"outputs":[],"source":["# Selecting Tensorflow version v2 (the command is relevant for Colab only).\n","%tensorflow_version 2.x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5376,"status":"ok","timestamp":1660106090612,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"nWnwUtAmLt1B","outputId":"a1b83773-5cee-4460-e48c-1b8811bc9fd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n","Collecting version\n","  Downloading version-0.1.1.tar.gz (2.0 kB)\n","\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/fd/b6/fa3b2c859d4d8817a106e4272029d78a2afbca0a27139997a4e5515bbf60/version-0.1.1.tar.gz#sha256=9ca4aae0312bc3924f4fd31cbfbef468ebf29f278174be0a6bb418fbfad7e85f (from https://pypi.org/simple/version/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n","  Downloading version-0.1.0.tar.gz (1.9 kB)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement 2.3.0 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for 2.3.0\u001b[0m\n","Python version: 3.7.13\n","Tensorflow version: 2.8.2\n","Keras version: 2.8.0\n"]}],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import platform\n","import datetime\n","import os\n","import math\n","import random\n","!pip install tensorflow version 2.3.0\n","print('Python version:', platform.python_version())\n","print('Tensorflow version:', tf.__version__)\n","print('Keras version:', tf.keras.__version__)"]},{"cell_type":"markdown","metadata":{"id":"eARhRY8PLt1u"},"source":["## Configuring TensorBoard\n","\n","We will use TensorBoard as a helper to debug the model training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLxzq_peLt1z","executionInfo":{"status":"ok","timestamp":1660104812414,"user_tz":300,"elapsed":126,"user":{"displayName":"waran kris","userId":"08591554211236964174"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8933f6a-d863-4f1a-8e65-9a34e28756dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}],"source":["# Load the TensorBoard notebook extension.\n","# %reload_ext tensorboard\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2ms5YLpLt13"},"outputs":[],"source":["# Clear any logs from previous runs.\n","!rm -rf ./logs/"]},{"cell_type":"markdown","metadata":{"id":"s9YXAy5kLt18"},"source":["## Loading the dataset\n","\n","We will download Rock-Paper-Scissors dataset from [TensorFlow Datasets](https://github.com/tensorflow/datasets) collection. To do that we loaded a `tensorflow_datasets` module.\n","\n","`tensorflow_datasets` defines a collection of datasets ready-to-use with TensorFlow.\n","\n","Each dataset is defined as a [tfds.core.DatasetBuilder](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetBuilder), which encapsulates the logic to download the dataset and construct an input pipeline, as well as contains the dataset documentation (version, splits, number of examples, etc.)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2zZOCweLt1-"},"outputs":[],"source":["# See available datasets\n","tfds.list_builders()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxDymXbpLt2D"},"outputs":[],"source":["DATASET_NAME = 'rock_paper_scissors'\n","\n","(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n","    name=DATASET_NAME,\n","    data_dir='tmp',\n","    with_info=True,\n","    as_supervised=True,\n","    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7791,"status":"ok","timestamp":1660104826872,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"JMRaImcqLt2H","outputId":"2a0ed7f1-aeec-4db3-d1b9-aebacca47026"},"outputs":[{"output_type":"stream","name":"stdout","text":["Raw train dataset: <PrefetchDataset element_spec=(TensorSpec(shape=(300, 300, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n","Raw train dataset size: 2520 \n","\n","Raw test dataset: <PrefetchDataset element_spec=(TensorSpec(shape=(300, 300, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n","Raw test dataset size: 372 \n","\n"]}],"source":["print('Raw train dataset:', dataset_train_raw)\n","print('Raw train dataset size:', len(list(dataset_train_raw)), '\\n')\n","\n","print('Raw test dataset:', dataset_test_raw)\n","print('Raw test dataset size:', len(list(dataset_test_raw)), '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1660104826873,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"w5pZ7fwnLt3V","outputId":"8e5c4c62-2b84-4537-bffb-c1f9c15ae64f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tfds.core.DatasetInfo(\n","    name='rock_paper_scissors',\n","    full_name='rock_paper_scissors/3.0.0',\n","    description=\"\"\"\n","    Images of hands playing rock, paper, scissor game.\n","    \"\"\",\n","    homepage='http://laurencemoroney.com/rock-paper-scissors-dataset',\n","    data_path='tmp/rock_paper_scissors/3.0.0',\n","    file_format=tfrecord,\n","    download_size=219.53 MiB,\n","    dataset_size=219.23 MiB,\n","    features=FeaturesDict({\n","        'image': Image(shape=(300, 300, 3), dtype=tf.uint8),\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n","    }),\n","    supervised_keys=('image', 'label'),\n","    disable_shuffling=False,\n","    splits={\n","        'test': <SplitInfo num_examples=372, num_shards=1>,\n","        'train': <SplitInfo num_examples=2520, num_shards=2>,\n","    },\n","    citation=\"\"\"@ONLINE {rps,\n","    author = \"Laurence Moroney\",\n","    title = \"Rock, Paper, Scissors Dataset\",\n","    month = \"feb\",\n","    year = \"2019\",\n","    url = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n","    }\"\"\",\n",")"]},"metadata":{},"execution_count":47}],"source":["dataset_info"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1660104827013,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"rvRfxFpN67yh","outputId":"ddd7100d-f3b8-4831-818e-7132f39a86de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of TRAIN examples: 2520\n","Number of TEST examples: 372\n","Number of label classes: 3\n"]}],"source":["NUM_TRAIN_EXAMPLES = dataset_info.splits['train'].num_examples\n","NUM_TEST_EXAMPLES = dataset_info.splits['test'].num_examples\n","NUM_CLASSES = dataset_info.features['label'].num_classes\n","\n","print('Number of TRAIN examples:', NUM_TRAIN_EXAMPLES)\n","print('Number of TEST examples:', NUM_TEST_EXAMPLES)\n","print('Number of label classes:', NUM_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1660104827013,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"yuvV63gyLt3b","outputId":"842c4168-cf44-46c0-bf0b-5d4ef30fc290"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input image size (original): 300\n","Input image shape (original): (300, 300, 3)\n","\n","\n","Input image size (reduced): 150\n","Input image shape (reduced): (150, 150, 3)\n","\n","\n","Input image size: 150\n","Input image shape: (150, 150, 3)\n"]}],"source":["INPUT_IMG_SIZE_ORIGINAL = dataset_info.features['image'].shape[0]\n","INPUT_IMG_SHAPE_ORIGINAL = dataset_info.features['image'].shape\n","\n","INPUT_IMG_SIZE_REDUCED = INPUT_IMG_SIZE_ORIGINAL // 2\n","INPUT_IMG_SHAPE_REDUCED = (\n","    INPUT_IMG_SIZE_REDUCED,\n","    INPUT_IMG_SIZE_REDUCED,\n","    INPUT_IMG_SHAPE_ORIGINAL[2]\n",")\n","\n","# Here we may switch between bigger or smaller image sized that we will train our model on.\n","INPUT_IMG_SIZE = INPUT_IMG_SIZE_REDUCED\n","INPUT_IMG_SHAPE = INPUT_IMG_SHAPE_REDUCED\n","\n","print('Input image size (original):', INPUT_IMG_SIZE_ORIGINAL)\n","print('Input image shape (original):', INPUT_IMG_SHAPE_ORIGINAL)\n","print('\\n')\n","print('Input image size (reduced):', INPUT_IMG_SIZE_REDUCED)\n","print('Input image shape (reduced):', INPUT_IMG_SHAPE_REDUCED)\n","print('\\n')\n","print('Input image size:', INPUT_IMG_SIZE)\n","print('Input image shape:', INPUT_IMG_SHAPE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbSOb5p4Lt3q"},"outputs":[],"source":["# Function to convert label ID to labels string.\n","get_label_name = dataset_info.features['label'].int2str"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660104827014,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"fef-jl8qLt3u","outputId":"d30e8623-fd8d-433a-92f6-8c540a7639b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["rock\n","paper\n","scissors\n"]}],"source":["print(get_label_name(0));\n","print(get_label_name(1));\n","print(get_label_name(2));"]},{"cell_type":"markdown","metadata":{"id":"--UTznQZLt5r"},"source":["## Exploring the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_MA0RMTLt5s"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","def preview_dataset(dataset):\n","    plt.figure(figsize=(12, 12))\n","    plot_index = 0\n","    for features in dataset.take(12):\n","        (image, label) = features\n","        print(image.numpy())\n","        plot_index += 1\n","        plt.subplot(3, 4, plot_index)\n","        # plt.axis('Off')\n","        label = get_label_name(label.numpy())\n","        plt.title('Label: %s' % label)\n","        plt.imshow(image.numpy())\n","def previewBW_dataset(dataset):\n","  for features in dataset.take(12):\n","    (image, label) = features\n","    imageArray = np.array(image, dtype=np.float32)\n","    cv2_imshow(imageArray)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-0-ntBmLt5x"},"outputs":[],"source":["# Explore raw training dataset images.\n","preview_dataset(dataset_train_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADqRAQ2YLt6E"},"outputs":[],"source":["# Explore what values are used to represent the image. \n","(first_image, first_lable) = list(dataset_train_raw.take(1))[0]\n","print('Label:', first_lable.numpy(), '\\n')\n","print('Image shape:', first_image.numpy().shape, '\\n')\n","print(type(first_image))\n","print(first_image.numpy())"]},{"cell_type":"markdown","metadata":{"id":"DuUdSSzoLt6I"},"source":["## Pre-processing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onNMq8CQLt6J"},"outputs":[],"source":["import cv2\n","def format_example(image, label):\n","    # Make image color values to be float.\n","    image = tf.cast(image, tf.float32)\n","    image = tf.image.resize(image, [INPUT_IMG_SIZE, INPUT_IMG_SIZE])\n","    image = tf.image.rgb_to_grayscale(image)\n","    return image, label\n","\n","def cv2_func(image, label):\n","    image = np.asarray(image).astype('uint8') # only way to convert tensor to np array is if the the tensor is an eager tensor. using np arrays is the only way to transform the image using cv2. But this destroys the shape of the tensorspec.\n","    #This contradictions makes it difficult to use the tensorflow dataset images to develop the model. Also the tf model for rock paper scissors only inputs rgb pictures, so our own dataset and model need to be developed.\n","    imageArray = cv2.GaussianBlur(image, (3,3), 0)\n","    imageArray = cv2.Canny(imageArray, 100, 150)\n","    tensor = tf.convert_to_tensor(imageArray, dtype=tf.float32)\n","    tensor = tf.expand_dims(tensor, -1)\n","    #print(tensor.shape)\n","    return image, label\n","\n","def tf_cv2_func(image, label):\n","    print(image.shape)\n","    (image, label) = tf.py_function(cv2_func, (image, label), [tf.float32, tf.int64])\n","    print(image.shape)\n","    return image, label\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUxjRhVpLt6M","executionInfo":{"status":"error","timestamp":1660106734418,"user_tz":300,"elapsed":312,"user":{"displayName":"waran kris","userId":"08591554211236964174"}},"colab":{"base_uri":"https://localhost:8080/","height":467},"outputId":"5769aade-6f54-45f9-db5c-63f1c6e8f770"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-157-038ec0f7a356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_test_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_cv2_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2015\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2016\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m       return ParallelMapDataset(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5195\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5196\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m     \"\"\"\n\u001b[1;32m   3070\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3071\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3072\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m       captured = object_identity.ObjectIdentitySet(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    246\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"<ipython-input-157-038ec0f7a356>\", line 2, in None  *\n        lambda x,y : (np.asarray(x).astype('uint8'), y))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"]}],"source":["dataset_train = dataset_train_raw.map(format_example)\n","dataset_train = dataset_train.map(tf_cv2_func)\n","dataset_test = dataset_test_raw.map(format_example)\n","dataset_test = dataset_test.map(tf_cv2_func)\n","\n","print(dataset_train,dataset_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7SYQ7kELt6U","colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"status":"error","timestamp":1660105372464,"user_tz":300,"elapsed":255,"user":{"displayName":"waran kris","userId":"08591554211236964174"}},"outputId":"9fbfcce7-ef2a-49af-bce9-f090373645a7"},"outputs":[{"output_type":"error","ename":"UnknownError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-d48fe5e5ab6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Explore preprocessed training dataset images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreviewBW_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-235f360fcfb0>\u001b[0m in \u001b[0;36mpreviewBW_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreviewBW_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimageArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    820\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2921\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2924\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: error: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/canny.cpp:829: error: (-215:Assertion failed) _src.depth() == CV_8U in function 'Canny'\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-95-2a42452c52d7>\", line 16, in cv2_func\n    imageArray = cv2.Canny(imageArray, 100, 150)\n\ncv2.error: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/canny.cpp:829: error: (-215:Assertion failed) _src.depth() == CV_8U in function 'Canny'\n\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]"]}],"source":["# Explore preprocessed training dataset images.\n","previewBW_dataset(dataset_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0QPUYQfLt6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660105054696,"user_tz":300,"elapsed":125,"user":{"displayName":"waran kris","userId":"08591554211236964174"}},"outputId":"875b59e6-b103-4902-99e9-734cd7ba311a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label: 2 \n","\n","Image shape: (150, 150, 1) \n","\n","[[[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," ...\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]]\n"]}],"source":["# Explore what values are used to represent the image. \n","(first_image, first_lable) = list(dataset_train.take(1))[0]\n","print('Label:', first_lable.numpy(), '\\n')\n","print('Image shape:', first_image.numpy().shape, '\\n')\n","print(first_image.numpy())"]},{"cell_type":"markdown","metadata":{"id":"Vojj1CCnLt6Y"},"source":["## Data augmentation\n","\n","One of the way to fight the [model overfitting](https://en.wikipedia.org/wiki/Overfitting) and to generalize the model to a broader set of examples is to augment the training data.\n","\n","As you saw from the previous section all training examples have a white background and vertically positioned right hands. But what if the image with the hand will be horizontally positioned or what if the background will not be that bright. What if instead of a right hand the model will see a left hand. To make our model a little bit more universal we're going to flip and rotate images and also to adjust background colors.   \n","\n","You may read more about a [Simple and efficient data augmentations using the Tensorfow tf.Data and Dataset API](https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsHN6_40_L-a"},"outputs":[],"source":["def augment_flip(image: tf.Tensor) -> tf.Tensor:\n","    image = tf.image.random_flip_left_right(image)\n","    image = tf.image.random_flip_up_down(image)\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nzkXAMa-5ZG"},"outputs":[],"source":["def augment_color(image: tf.Tensor) -> tf.Tensor:\n","    image = tf.image.random_hue(image, max_delta=0.08)\n","    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n","    image = tf.image.random_brightness(image, 0.05)\n","    image = tf.image.random_contrast(image, lower=0.8, upper=1)\n","    image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeF-o0X0ARj0"},"outputs":[],"source":["def augment_rotation(image: tf.Tensor) -> tf.Tensor:\n","    # Rotate 0, 90, 180, 270 degrees\n","    return tf.image.rot90(\n","        image,\n","        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SK9mlrfVua7L"},"outputs":[],"source":["def augment_inversion(image: tf.Tensor) -> tf.Tensor:\n","    random = tf.random.uniform(shape=[], minval=0, maxval=1)\n","    if random > 0.5:\n","        image = tf.math.multiply(image, -1)\n","        image = tf.math.add(image, 1)\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfVxVWSOAyPq"},"outputs":[],"source":["def augment_zoom(image: tf.Tensor, min_zoom=0.8, max_zoom=1.0) -> tf.Tensor:\n","    image_width, image_height, image_colors = image.shape\n","    crop_size = (image_width, image_height)\n","\n","    # Generate crop settings, ranging from a 1% to 20% crop.\n","    scales = list(np.arange(min_zoom, max_zoom, 0.01))\n","    boxes = np.zeros((len(scales), 4))\n","\n","    for i, scale in enumerate(scales):\n","        x1 = y1 = 0.5 - (0.5 * scale)\n","        x2 = y2 = 0.5 + (0.5 * scale)\n","        boxes[i] = [x1, y1, x2, y2]\n","\n","    def random_crop(img):\n","        # Create different crops for an image\n","        crops = tf.image.crop_and_resize(\n","            [img],\n","            boxes=boxes,\n","            box_indices=np.zeros(len(scales)),\n","            crop_size=crop_size\n","        )\n","        # Return a random crop\n","        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n","\n","    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n","\n","    # Only apply cropping 50% of the time\n","    return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eD9cluIhLt6Y"},"outputs":[],"source":["def augment_data(image, label):\n","    image = augment_flip(image)\n","    #image = augment_color(image)\n","    image = augment_rotation(image)\n","    #image = augment_zoom(image)\n","    image = augment_inversion(image)\n","    return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eceiqwBKLt72"},"outputs":[],"source":["dataset_train_augmented = dataset_train.map(augment_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRyHTdQhLt8C"},"outputs":[],"source":["# Explore augmented training dataset.\n","\n","print(dataset_train)\n","previewBW_dataset(dataset_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PliJJvraV0w-"},"outputs":[],"source":["# Explore test dataset.\n","previewBW_dataset(dataset_test)"]},{"cell_type":"markdown","metadata":{"id":"GBTbwiPYLt8G"},"source":["## Data shuffling and batching\n","\n","We don't want our model to learn anything from the order or grouping of the images in the dataset. To avoid that we will shuffle the training examples. Also we're going to split the training set by batches to speed up training process and make it less memory consuming."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCWeWR5QLt8H"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","dataset_train_augmented_shuffled = dataset_train.shuffle(\n","    buffer_size=NUM_TRAIN_EXAMPLES\n",")\n","\n","dataset_train_augmented_shuffled = dataset_train.batch(\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Prefetch will enable the input pipeline to asynchronously fetch batches while your model is training.\n","dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.prefetch(\n","    buffer_size=tf.data.experimental.AUTOTUNE\n",")\n","\n","dataset_test_shuffled = dataset_test.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1660105064247,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"TiXw2QMxLt8K","outputId":"6952ecc0-5265-4106-8ca7-189ef83e07e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))>\n","<BatchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))>\n"]}],"source":["print(dataset_train_augmented_shuffled)\n","print(dataset_test_shuffled)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1660105086605,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"KBQW_0i5Lt8W","outputId":"efe3631a-d0a6-4953-90cd-8f16aa9055fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label batch shape: (32,) \n","\n","Image batch shape: (32, 150, 150, 1) \n","\n","Label batch: [2 2 0 1 0 1 2 1 2 2 1 1 2 1 1 1 1 1 1 1 1 0 0 0 0 1 1 2 2 2 0 0] \n","\n","First batch image: [[[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," ...\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]] \n","\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=150x150 at 0x7F49EFD4FD50>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAADzklEQVR4nO2d2ZKjOhQExcT9/19mHnyDINhGAi1VR5lP7R73WJDU0YJspwQAAAAAAAAAxiyjGyDKuq7bz8vCWbJiXde9v8NDUOdSGBaduFOlrPDP6AYI8exJ2SL8j6lCUghR2EJm1x2SwgsOOmXl/UDhkd9EfptIbP6WZdF0+d/oBijitRxDCiEEmRVSs5CSQntQaI/6cOZcu7zGGh2QVvjzt3f2G+tjcY96Id1maZcPISkrvEsbFg/oKnyAQrrHUuGPzkGU7YM9FJ7XJzXP5hCkR6SJPs+aHHm1BPd8rep4FFJ4wFuh7D28nngrBHX6dFHWd5pSgBRSS+0VdkP2WkGhPREUyuajD9IKJ3eTibRCyCGIwpnzGkThzKgrLIrXnEFUV5hPrTuI647zS8x5lXylKIWlp7j18ztgkML8a3/OW/kGCksRDEpTPBQSxAc8FBYx26AjoMLZsFE4W7ZiUqSwxb14zWvIJoVwh5NCauklTgqLmMd3WIWNmOSyaEvdAciLNdK7RXDIpaLCjxpERFoWUpGZgMj7jf0UFq2Ctl4yVbDop/AFTU/x8IV1S4VFE4bhp7g1lgpf0LrWDaylxldo0ecXnJ98/lyi7+05/CZ8AfhK6bh0//wO84FuUw7vy6Q0iPuHHSJSPegBGT4n+ycdsjjLcGYUChNHdSzOTtNGkkIQYPIgkkJ7UAgazFxLSSFoYJHC1KadpNAeFIIGLoU0NWgqKQQZXIJICuEICu1BIShh0R369YUWp9Wa+t8Wc95ldGeRfUFVqKnwbsPWnarDtxLCO6opfLHhbtsahMsv1FH4ZcPk4Xtez78sagO8ofpWyaL/0O49t4pNbdGmHDFe5jb8JhXvWJbl3FPuYa/7RuXhzMbDKDT/vG/P3PeRjH3qc5eS5wC9eyHT4rlHsf0vTmsAE6/RPfB1R/6fNG2SJtWPulpfeDm9O/9TYlHGjstozpm/1ObAm38p+nlUmcifO4xlIhBjelBKwOOdTWTYI53HYqPDHL9G+rwWCk7Ettju0MancIPP93iHkMJEUY1EPItTFNI9FNV8RBUmLGajqzBhMQ9phYkBTiTcLVo3vhq+Fps2W72Q7qFrvMRJYcLiFWYKExZP+ClMDFMj4WKR4cwtFNXkrjBRVCOhbJFCmgVxjIOgRVJYBmOcIKhlcbq79t+ZJ4thFaaZLAZHZ9u/QhuMUbAYdkN+HyiqQRiusEUDZkmhDtUtorArLd6hjsIB1A0iCntTPYgoBCuGj0g3KraEFNqDQntQaA8K7UGhPSgEK3QmFaleY0ihPX8B5XQf86AORAMAAAAASUVORK5CYII=\n"},"metadata":{}}],"source":["# Debugging the batches using conversion to Numpy arrays.\n","batches = tfds.as_numpy(dataset_train_augmented_shuffled)\n","for batch in batches:\n","    image_batch, label_batch = batch\n","    print('Label batch shape:', label_batch.shape, '\\n')\n","    print('Image batch shape:', image_batch.shape, '\\n')\n","    print('Label batch:', label_batch, '\\n')\n","    \n","    for batch_item_index in range(len(image_batch)):\n","        print('First batch image:', image_batch[batch_item_index], '\\n')\n","        cv2_imshow(image_batch[batch_item_index])\n","        #plt.imshow(image_batch[batch_item_index])\n","        #plt.show()\n","        # Break to shorten the output.\n","        break\n","    # Break to shorten the output.\n","    break"]},{"cell_type":"markdown","metadata":{"id":"Hz_hZSCULt8Z"},"source":["## Creating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4cHfXyQLt8a"},"outputs":[],"source":["model = tf.keras.models.Sequential()\n","\n","# First convolution.\n","model.add(tf.keras.layers.Convolution2D(\n","    input_shape=(150,150,1),\n","    filters=64,\n","    kernel_size=3,\n","    activation=tf.keras.activations.relu\n","))\n","model.add(tf.keras.layers.MaxPooling2D(\n","    pool_size=(2, 2),\n","    strides=(2, 2)\n","))\n","\n","# Second convolution.\n","model.add(tf.keras.layers.Convolution2D(\n","    filters=64,\n","    kernel_size=3,\n","    activation=tf.keras.activations.relu\n","))\n","model.add(tf.keras.layers.MaxPooling2D(\n","    pool_size=(2, 2),\n","    strides=(2, 2)\n","))\n","\n","# Third convolution.\n","model.add(tf.keras.layers.Convolution2D(\n","    filters=128,\n","    kernel_size=3,\n","    activation=tf.keras.activations.relu\n","))\n","model.add(tf.keras.layers.MaxPooling2D(\n","    pool_size=(2, 2),\n","    strides=(2, 2)\n","))\n","\n","# Fourth convolution.\n","model.add(tf.keras.layers.Convolution2D(\n","    filters=128,\n","    kernel_size=3,\n","    activation=tf.keras.activations.relu\n","))\n","model.add(tf.keras.layers.MaxPooling2D(\n","    pool_size=(2, 2),\n","    strides=(2, 2)\n","))\n","\n","# Flatten the results to feed into dense layers.\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dropout(0.5))\n","\n","# 512 neuron dense layer.\n","model.add(tf.keras.layers.Dense(\n","    units=512,\n","    activation=tf.keras.activations.relu\n","))\n","\n","# Output layer.\n","model.add(tf.keras.layers.Dense(\n","    units=NUM_CLASSES,\n","    activation=tf.keras.activations.softmax\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2Kp94HfLt8e"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGiEAOgWLt8m"},"outputs":[],"source":["tf.keras.utils.plot_model(\n","    model,\n","    show_shapes=True,\n","    show_layer_names=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"oJcmE__0Lt8r"},"source":["## Compiling the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJ8jGFnTLt8t"},"outputs":[],"source":["# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n","\n","model.compile(\n","    optimizer='adam',\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    metrics=['accuracy']\n",")"]},{"cell_type":"markdown","metadata":{"id":"p76LZoDiLt9r"},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1660105125771,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"yZJIp8qELt9s","outputId":"dd0723b3-8c40-4a8e-abc7-ad34fdcb7e13"},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch: 78\n","validation_steps: 11\n"]}],"source":["steps_per_epoch = NUM_TRAIN_EXAMPLES // BATCH_SIZE\n","validation_steps = NUM_TEST_EXAMPLES // BATCH_SIZE\n","\n","print('steps_per_epoch:', steps_per_epoch)\n","print('validation_steps:', validation_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D2etjoyPZTE"},"outputs":[],"source":["!rm -rf tmp/checkpoints\n","!rm -rf logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vG95ZXoLLt92"},"outputs":[],"source":["# Preparing callbacks.\n","os.makedirs('logs/fit', exist_ok=True)\n","tensorboard_log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(\n","    log_dir=tensorboard_log_dir,\n","    histogram_freq=1\n",")\n","\n","os.makedirs('tmp/checkpoints', exist_ok=True)\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath='tmp/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",")\n","\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n","    patience=5,\n","    monitor='val_accuracy'\n","    # monitor='val_loss'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":779},"executionInfo":{"elapsed":415,"status":"error","timestamp":1660105129361,"user":{"displayName":"waran kris","userId":"08591554211236964174"},"user_tz":300},"id":"8ppHB7fNLt-a","outputId":"477bc6db-cac1-4f16-c998-f6050fe70175"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-83-9687bbd67daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 864, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 957, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 438, in update_state\n        self.build(y_pred, y_true)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 359, in build\n        self._metrics, y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 484, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 484, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 505, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"]}],"source":["training_history = model.fit(\n","    x=dataset_train_augmented_shuffled.repeat(),\n","    validation_data=dataset_test_shuffled.repeat(),\n","    epochs=15,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_steps=validation_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbT9WYOTOGme"},"outputs":[],"source":["def render_training_history(training_history):\n","    loss = training_history.history['loss']\n","    val_loss = training_history.history['val_loss']\n","\n","    accuracy = training_history.history['accuracy']\n","    val_accuracy = training_history.history['val_accuracy']\n","\n","    plt.figure(figsize=(14, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.title('Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.plot(loss, label='Training set')\n","    plt.plot(val_loss, label='Test set', linestyle='--')\n","    plt.legend()\n","    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n","\n","    plt.subplot(1, 2, 2)\n","    plt.title('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.plot(accuracy, label='Training set')\n","    plt.plot(val_accuracy, label='Test set', linestyle='--')\n","    plt.legend()\n","    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSXmInPTLt-n","scrolled":false},"outputs":[],"source":["render_training_history(training_history)"]},{"cell_type":"markdown","metadata":{"id":"Ubd3vQXuLt-v"},"source":["## Debugging the training with TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxHx2Bu9Lt-v"},"outputs":[],"source":["%tensorboard --logdir logs/fit"]},{"cell_type":"markdown","metadata":{"id":"wmcefwvctykA"},"source":["## Evaluating model accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwA0C3ZEt1X-"},"outputs":[],"source":["# %%capture\n","train_loss, train_accuracy = model.evaluate(\n","    x=dataset_train.batch(BATCH_SIZE).take(NUM_TRAIN_EXAMPLES)\n",")\n","\n","test_loss, test_accuracy = model.evaluate(\n","    x=dataset_test.batch(BATCH_SIZE).take(NUM_TEST_EXAMPLES)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CikvwUBwUVZ"},"outputs":[],"source":["print('Training loss: ', train_loss)\n","print('Training accuracy: ', train_accuracy)\n","print('\\n')\n","print('Test loss: ', test_loss)\n","print('Test accuracy: ', test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"1dOzgOVT1KFd"},"source":["## Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3APy_0-1LvQ"},"outputs":[],"source":["model_name = 'rock_paper_scissors_cnn.h5'\n","model.save(model_name, save_format='h5')"]},{"cell_type":"markdown","metadata":{"id":"kpRWiLQy-sd5"},"source":["## Converting the model to web-format\n","\n","To use this model on the web we need to convert it into the format that will be understandable by [tensorflowjs](https://www.tensorflow.org/js). To do so we may use [tfjs-converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter) as following:\n","\n","```\n","tensorflowjs_converter --input_format keras \\\n","  ./experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.h5 \\\n","  ./demos/public/models/rock_paper_scissors_cnn\n","```\n","\n","You find this experiment in the [Demo app](https://trekhleb.github.io/machine-learning-experiments) and play around with it right in you browser to see how the model performs in real life."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Copy of rock_paper_scissors_cnn1.ipynb","provenance":[{"file_id":"1IIoi2UZhaLZkVmzJ7uoPGBAkoC9dEX60","timestamp":1660104651599},{"file_id":"https://github.com/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb","timestamp":1660092446605}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}